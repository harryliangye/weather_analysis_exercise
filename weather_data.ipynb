{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61dfe51b-3512-4904-a0c9-a9acbe19e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr, col, regexp_replace\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a5fc79-b933-4b4e-8327-9a0d552da40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Preparation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cddd5259-aa74-4502-b924-06511ccefcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function design\n",
    "def load_data_as_sqlview(table_name: str, file_pattern: str, spark: SparkSession):\n",
    "    # Use the specified file_pattern to read the files\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_pattern)\n",
    "    # Create a temporary SQL view for the DataFrame\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "    return df\n",
    "\n",
    "def run_and_show(spark, query, lines=3):\n",
    "    result = spark.sql(query)\n",
    "    result.show(lines)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bb3503-d61e-46b3-90b4-861b2b8673a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1425fada-0960-4bcc-aa9a-171c71931781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a1fd01-d5a0-4136-b544-8c4e6c6d2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_reading = load_data_as_sqlview(\n",
    "    table_name=\"station_reading\",\n",
    "    file_pattern = \"/home/jovyan/data/2019/part-*.csv.gz\",\n",
    "    spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5e8c4c-b1bd-493d-8261-042963e94c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = load_data_as_sqlview(\n",
    "    table_name=\"station_list\",\n",
    "    file_pattern = \"/home/jovyan/stationlist.csv\",\n",
    "    spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81c024a-61fd-470b-ba2d-688a09a04e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = load_data_as_sqlview(\n",
    "    table_name=\"country_list\",\n",
    "    file_pattern = \"/home/jovyan/countrylist.csv\",\n",
    "    spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f723a6b0-116f-4ce0-b2d1-4a962a72a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "|STN---| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "| 10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|\n",
      "| 10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|\n",
      "| 10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "station_reading.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e486d8-a9e3-414e-8fea-75fefd340054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column name\n",
    "station_reading = station_reading.withColumnRenamed(\"STN---\", \"STN_NO_RD\")\n",
    "station_reading.createOrReplaceTempView('station_reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad62a75f-8218-4f83-bcac-5d131dfa317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|012240|          NO|\n",
      "|020690|          SW|\n",
      "|020870|          SW|\n",
      "+------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "station_list.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7b13b4-b78f-4ffd-890b-2f6baf2cbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|COUNTRY_ABBR|       COUNTRY_FULL|\n",
      "+------------+-------------------+\n",
      "|          AA|              ARUBA|\n",
      "|          AC|ANTIGUA AND BARBUDA|\n",
      "|          AF|        AFGHANISTAN|\n",
      "+------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "country_list.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90dc10a6-aaae-4a24-8f1d-0a3b05500bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames using DataFrame operations\n",
    "station_reading_full = station_reading\\\n",
    "    .join(station_list,  station_list[\"STN_NO\"].cast(\"int\") == station_reading[\"STN_NO_RD\"].cast(\"int\"), how=\"left\")\\\n",
    "    .join(country_list, station_list[\"COUNTRY_ABBR\"] == country_list[\"COUNTRY_ABBR\"], how=\"left\")\n",
    "station_reading_full.createOrReplaceTempView('station_reading_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef646d0-9ab4-428f-8df7-052cf94b7c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------+------------+------------+------------+\n",
      "|STN_NO_RD| WBAN|YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST|  MAX|  MIN| PRCP| SNDP|FRSHTT|STN_NO|COUNTRY_ABBR|COUNTRY_ABBR|COUNTRY_FULL|\n",
      "+---------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------+------------+------------+------------+\n",
      "|    10260|99999|20190101|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7| 29.8|21.7*|0.02G| 18.5|  1000|010260|          NO|          NO|      NORWAY|\n",
      "|    10260|99999|20190102|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|27.1*| 20.7|0.48G| 22.8|  1000|010260|          NO|          NO|      NORWAY|\n",
      "|    10260|99999|20190103|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|37.4*|26.8*|0.25G|999.9| 11000|010260|          NO|          NO|      NORWAY|\n",
      "+---------+-----+--------+----+----+------+------+-----+----+-----+----+-----+-----+-----+-----+------+------+------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[STN_NO_RD: int, WBAN: int, YEARMODA: int, TEMP: double, DEWP: double, SLP: double, STP: double, VISIB: double, WDSP: double, MXSPD: double, GUST: double, MAX: string, MIN: string, PRCP: string, SNDP: double, FRSHTT: int, STN_NO: string, COUNTRY_ABBR: string, COUNTRY_ABBR: string, COUNTRY_FULL: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data\n",
    "query = \"SELECT * FROM station_reading_full LIMIT 3\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0ef32c0-c298-4781-befc-2bc2dc081b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  slo|   sr|\n",
      "+-----+-----+\n",
      "|12144|12144|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[slo: bigint, sr: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check join\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT STN_NO) AS slo,\n",
    "    COUNT(DISTINCT STN_NO_RD) AS sr\n",
    "FROM station_reading_full;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46d5b84-c855-47f3-bc52-de9cbffc5e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|STN_NO|STN_NO_RD|\n",
      "+------+---------+\n",
      "|043390|    43390|\n",
      "|013800|    13800|\n",
      "|020950|    20950|\n",
      "+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[STN_NO: string, STN_NO_RD: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check join\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT STN_NO, STN_NO_RD\n",
    "FROM station_reading_full\n",
    "WHERE STN_NO_RD IS NOT NULL \n",
    "    AND STN_NO IS NOT NULL \n",
    "    AND CAST(STN_NO AS STRING) <> CAST(STN_NO_RD AS STRING)\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a562582-d593-4748-8d0d-3012fb61f9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|STN_NO|STN_NO_RD|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[STN_NO: string, STN_NO_RD: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check join\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT STN_NO, STN_NO_RD\n",
    "FROM station_reading_full\n",
    "WHERE STN_NO_RD IS NOT NULL \n",
    "    AND STN_NO IS NULL\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1df0334-bac4-4650-b464-5b1de5016cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Answering Question Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39f8742-f39f-417f-b7ba-88c1211ba1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Which country had the hottest average mean temperature over the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbce1e86-4966-438b-9571-bf02017591d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|max_date|min_date|\n",
      "+--------+--------+\n",
      "|20200101|20190101|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[max_date: int, min_date: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check date, to make sure we only have 1 year of data\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    MAX(YEARMODA) AS max_date,\n",
    "    MIN(YEARMODA) AS min_date\n",
    "FROM station_reading_full\n",
    "LIMIT 3;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81186943-a034-4959-9f2e-b24f21b77eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|COUNTRY_FULL|    avg_mean_temp|\n",
      "+------------+-----------------+\n",
      "|    DJIBOUTI|90.06114457831323|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[COUNTRY_FULL: string, avg_mean_temp: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH mt AS (\n",
    "    SELECT COUNTRY_FULL, YEARMODA,  AVG(TEMP) AS mean_temp\n",
    "    FROM station_reading_full\n",
    "    WHERE \n",
    "        YEARMODA BETWEEN 20190101 AND 20191231\n",
    "        AND TEMP <> 9999.9\n",
    "    GROUP BY COUNTRY_FULL, YEARMODA\n",
    "    )\n",
    "    \n",
    "SELECT \n",
    "    COUNTRY_FULL, \n",
    "    AVG(mean_temp) AS avg_mean_temp\n",
    "FROM mt\n",
    "GROUP BY COUNTRY_FULL\n",
    "ORDER BY AVG(mean_temp) DESC\n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91332d78-487d-4479-b436-53831fa3917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Which country had the most consecutive days of tornadoes/funnel cloud formations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63e2515-22ac-4c89-bca7-6a80add78532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|len(FRSHTT)|    cnt|\n",
      "+-----------+-------+\n",
      "|          1|2807103|\n",
      "|          2|  33583|\n",
      "|          4| 188837|\n",
      "|          5| 897708|\n",
      "|          6| 234103|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[len(FRSHTT): int, cnt: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT LEN(FRSHTT),\n",
    "    COUNT(1) AS cnt\n",
    "FROM station_reading_full\n",
    "GROUP BY LEN(FRSHTT)\n",
    "ORDER BY LEN(FRSHTT);\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query, lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe35e91-cb26-4797-ba89-33023fc457fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+\n",
      "|COUNTRY_FULL|YEARMODA|tornado|\n",
      "+------------+--------+-------+\n",
      "| AFGHANISTAN|20190101|      0|\n",
      "| AFGHANISTAN|20190102|      0|\n",
      "| AFGHANISTAN|20190103|      0|\n",
      "+------------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNTRY_FULL,\n",
    "    YEARMODA,\n",
    "    MAX(CASE WHEN LEN(FRSHTT) = 6 AND SUBSTRING(FRSHTT, 6, 1) = '1' THEN 1 ELSE 0 END) AS tornado\n",
    "FROM station_reading_full\n",
    "WHERE COUNTRY_FULL IS NOT NULL\n",
    "GROUP BY COUNTRY_FULL, YEARMODA\n",
    "ORDER BY COUNTRY_FULL, YEARMODA ASC\n",
    ";\n",
    "\"\"\"\n",
    "df_q3 = run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf02dd0-cf8a-4faf-a45a-6584cd47e3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'AUSTRIA, BAHAMAS THE, CANADA, COLOMBIA, CUBA, GEORGIA, GHANA, ICELAND, IRAN, ITALY, MADAGASCAR, NEPAL, NORWAY, POLAND, ROMANIA, RUSSIA, SPAIN, TURKEY, UNITED STATES',\n",
       " 'tor_day': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_consec_tornado_per_country(spark_df):\n",
    "    country_stat = dict()  # country_name -> max consecutive tornado days\n",
    "    df = spark_df.collect()  # Convert to a local Pandas DataFrame\n",
    "    current_country = \"\"\n",
    "    tor_days = 0\n",
    "    for row in df:\n",
    "        country_name = row['COUNTRY_FULL']\n",
    "        tor = row['tornado']\n",
    "        if country_name != current_country:\n",
    "            if country_name not in country_stat:\n",
    "                country_stat[country_name] = 0\n",
    "            if current_country != \"\":\n",
    "                country_stat[current_country] = tor_days if tor_days > country_stat[current_country] else country_stat[current_country]\n",
    "            tor_days = 0\n",
    "            current_country = country_name\n",
    "        if tor == 1:\n",
    "            tor_days += 1\n",
    "        else:\n",
    "            country_stat[country_name] = tor_days if tor_days > country_stat[country_name] else country_stat[country_name]\n",
    "            tor_days = 0\n",
    "    return country_stat\n",
    "\n",
    "def find_max_country(country_max):\n",
    "    max_tor = {\n",
    "        'country': \"\",\n",
    "        'tor_day': -1\n",
    "    }\n",
    "    for c_name in country_max:\n",
    "        if country_max[c_name] > max_tor['tor_day']:\n",
    "            max_tor['country'] = c_name\n",
    "            max_tor['tor_day'] = country_max[c_name]\n",
    "        elif country_max[c_name] == max_tor['tor_day']:\n",
    "            max_tor['country'] += f\", {c_name}\"\n",
    "    return max_tor\n",
    "\n",
    "country_max = find_consec_tornado_per_country(df_q3)\n",
    "find_max_country(country_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58110ee5-afcb-41fa-8442-2e26e2ea8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max_consec|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[max_consec: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To validate the answer, use the following SQL query to see if there's any country that has at least two(2) consecutive tornado days\n",
    "query = \"\"\"\n",
    "WITH ct AS (\n",
    "    SELECT \n",
    "        COUNTRY_FULL,\n",
    "        YEARMODA,\n",
    "        MAX(CASE WHEN LEN(FRSHTT) = 6 AND SUBSTRING(FRSHTT, 6, 1) = '1' THEN 1 ELSE 0 END) AS tornado\n",
    "    FROM station_reading_full\n",
    "    WHERE COUNTRY_FULL IS NOT NULL\n",
    "    GROUP BY COUNTRY_FULL, YEARMODA\n",
    "    ORDER BY COUNTRY_FULL, YEARMODA ASC\n",
    "    ),\n",
    "    \n",
    "    cs AS (\n",
    "    SELECT \n",
    "        COUNTRY_FULL,\n",
    "        YEARMODA,\n",
    "        SUM(tornado) OVER (PARTITION BY COUNTRY_FULL ORDER BY YEARMODA ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS consec\n",
    "    FROM ct\n",
    "    )\n",
    "    \n",
    "SELECT MAX(consec) AS max_consec\n",
    "FROM cs;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac34471b-72c2-45b5-a61a-bbb4484e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Which country had the second highest average mean wind speed over the year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d4b013-d013-4a07-b950-772cb4a70996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+----+\n",
      "|COUNTRY_FULL|     avg_mean_wdsp|rank|\n",
      "+------------+------------------+----+\n",
      "|       ARUBA|15.981917808219182|   2|\n",
      "+------------+------------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[COUNTRY_FULL: string, avg_mean_wdsp: double, rank: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH mt AS (\n",
    "    SELECT COUNTRY_FULL, YEARMODA,  AVG(WDSP) AS mean_wdsp\n",
    "    FROM station_reading_full\n",
    "    WHERE \n",
    "        YEARMODA BETWEEN 20190101 AND 20191231\n",
    "        AND WDSP <> 999.9\n",
    "    GROUP BY COUNTRY_FULL, YEARMODA\n",
    "    ),\n",
    "    \n",
    "    rk AS (SELECT \n",
    "        COUNTRY_FULL, \n",
    "        AVG(mean_wdsp) AS avg_mean_wdsp,\n",
    "        ROW_NUMBER() OVER (ORDER BY AVG(mean_wdsp) DESC) AS rank\n",
    "    FROM mt\n",
    "    GROUP BY COUNTRY_FULL)\n",
    "\n",
    "SELECT *\n",
    "FROM rk\n",
    "WHERE rank = 2;\n",
    "\"\"\"\n",
    "run_and_show(spark=spark, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e6f12-34ab-4546-8130-0822dc3fb4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1112d9df-3e1b-420d-aaa9-2ddd8ee4353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m stations \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTN_NO\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m station_reading_full\u001b[38;5;241m.\u001b[39mfilter(station_reading_full[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRY_FULL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m country)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTN_NO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m stations:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# If there's a missing TEMP value (999.9) within the group for the station\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mgroup_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9999.9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m         \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Features for regression (all stations except the current one)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         features \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m stations \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m!=\u001b[39m station]\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Impute temperature data  [Takes long time, not applying this at the moment]\n",
    "spark.conf.set(\"spark.sql.pivotMaxValues\", \"50000\")\n",
    "\n",
    "# Pivot the DataFrame to have a column for TEMP value for each station for each day within a country\n",
    "pivot_df = station_reading_full.groupBy(\"COUNTRY_FULL\", \"YEARMODA\").pivot(\"STN_NO\").agg(F.first(\"TEMP\"))\n",
    "\n",
    "# Placeholder for the results\n",
    "imputed_rows = []\n",
    "\n",
    "# Get the list of unique countries\n",
    "countries = [row['COUNTRY_FULL'] for row in pivot_df.select(\"COUNTRY_FULL\").distinct().collect()]\n",
    "\n",
    "# Loop through each country and impute missing values for each station using regression\n",
    "for country in countries:\n",
    "    group_df = pivot_df.filter(pivot_df[\"COUNTRY_FULL\"] == country)\n",
    "    stations = [row['STN_NO'] for row in station_reading_full.filter(station_reading_full[\"COUNTRY_FULL\"] == country).select(\"STN_NO\").distinct().collect()]\n",
    "    for station in stations:\n",
    "        # If there's a missing TEMP value (999.9) within the group for the station\n",
    "        if group_df.filter(group_df[station] == 9999.9).count() > 0:\n",
    "            \n",
    "            # Features for regression (all stations except the current one)\n",
    "            features = [s for s in stations if s != station]\n",
    "\n",
    "            # Prepare the data\n",
    "            assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "            assembled_df = assembler.transform(group_df).na.drop(subset=[\"features\"])\n",
    "\n",
    "            # Separate training and testing data\n",
    "            train_df = assembled_df.filter(assembled_df[station] != 999.9)\n",
    "            test_df = assembled_df.filter(assembled_df[station] == 999.9)\n",
    "\n",
    "            # Linear regression model\n",
    "            lr = LinearRegression(featuresCol=\"features\", labelCol=station)\n",
    "            model = lr.fit(train_df)\n",
    "\n",
    "            # Predict the missing TEMP values\n",
    "            predictions = model.transform(test_df)\n",
    "            imputed_rows.extend(predictions.select(\"COUNTRY_FULL\", \"YEARMODA\", station).collect())\n",
    "\n",
    "# Convert the imputed rows back to the original format\n",
    "converted_rows = []\n",
    "\n",
    "for row in imputed_rows:\n",
    "    # Extracting station number from column name\n",
    "    station_no = [col for col in row.asDict().keys() if col.isdigit()][0]\n",
    "    country = row[\"COUNTRY_FULL\"]\n",
    "    date = row[\"YEARMODA\"]\n",
    "    temp_value = row[station_no]\n",
    "    converted_rows.append((country, date, station_no, temp_value))\n",
    "\n",
    "# Convert the list of tuples to a DataFrame\n",
    "imputed_df = spark.createDataFrame(converted_rows, [\"COUNTRY_FULL\", \"YEARMODA\", \"STN_NO\", \"TEMP\"])\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "imputed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c1458-a6b0-41b1-9d41-a6d784a0a60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
